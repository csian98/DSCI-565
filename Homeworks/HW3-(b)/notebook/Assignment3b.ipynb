{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d143d8-958b-4833-8f6b-50ee297072b6",
   "metadata": {},
   "source": [
    "<h1>Assignment 3b: Physics-Informed Neural Networks</h1>\n",
    "In this part of the assignment, you'll implement a vanilla PINN (Physics-Informed Neural Network).  We follow closely the tutorial in  https://arxiv.org/pdf/2403.00599v1, Section 2.\n",
    "\n",
    "The Laplace equation is a partial differential equation (PDE) ubiquitous in physics and mathematics, used for describing phenomena in gravitation, fluid flow, electromagnetism, among others.  The equation in 2 dimensions is\n",
    "\n",
    "$$\\frac{\\partial^2}{\\partial x^2}u(x,y) + \\frac{\\partial^2}{\\partial y^2}u(x,y) = 0$$\n",
    "\n",
    "where $u:\\mathbb{R}^2 \\to \\mathbb{R}$ is a smooth function defined on a subset of $\\mathbb{R}$.\n",
    "\n",
    "\n",
    "For a given domain $\\Omega\\subset \\mathbb{R}^2$ and a fixed function $f:\\partial\\Omega\\to \\mathbb{R}$ defining the value of $u$ on the boundary of the domain, a common task in scientific computing is to find a function $u:\\Omega\\to \\mathbb{R}$ satifying the Laplace equation, i.e. a $u$ so that\n",
    "$$\\frac{\\partial^2}{\\partial x^2}u(x,y) + \\frac{\\partial^2}{\\partial y^2}u(x,y) = 0 \\quad\\quad\\forall (x,y) \\in \\Omega$$\n",
    "$$u(x,y) = f(x,y) \\quad\\quad\\forall (x,y) \\in \\partial\\Omega$$\n",
    "\n",
    "PINN is an approach which defines a small neural network representing $u$, and a loss which encourages it to solve these equations.  The hope is that by training the neural network, we can recover a solution to the PDE for given boundary conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e9fa1-250d-4b03-8f85-6da01a692bf8",
   "metadata": {},
   "source": [
    "<h3>Question 1</h3>\n",
    "Show (provide an analytic solution) that the function $u(x,y)=x^2-y^2$ satisfies the Laplace equation $\\forall (x,y) \\in \\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148daa0-4da8-4f14-8ead-85c539ca6521",
   "metadata": {},
   "source": [
    "###\n",
    "\\#\\#\\#\\#\\# YOUR SOLUTION TYPED HERE\\#\\#\\#\\#\\#\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63005b64-d6e5-41b2-bf0b-b1e974d0bfaa",
   "metadata": {},
   "source": [
    "<h2>Network Definition</h2>\n",
    "<h3>Question 2</h3>\n",
    "Define a neural network with\n",
    "<ul>\n",
    "    <li>Input: a tensor of input size 2</li>\n",
    "    <li>Output: a tensor of output size 1</li>\n",
    "    <li>5 hidden layers with 20 neurons each</li>\n",
    "    <li>tanh activation functions for the hidden layers</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a065a88a-d7ea-4bac-84f9-98886aca73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.qmc import LatinHypercube\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        ##### DEFINE YOUR LAYERS HERE #####\n",
    "\n",
    "        \n",
    "        \n",
    "        ##### ----------------------- #####\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##### DEFINE THE FORWARD PART OF YOUR NETWORK HERE #####\n",
    "\n",
    "        \n",
    "        \n",
    "        ##### -------------------------------------------- #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c5c1c8-cf23-43ce-b9b9-a25873edb960",
   "metadata": {},
   "source": [
    "<h2>Boundary Loss Definition</h2>\n",
    "To enforce the boundary conditions, we sample points on the boundary of the domain and use the mean squared error vs the fixed boundary conditions we want to enforce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1adb9-99fd-4b71-8d8f-b699f0a02ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bdry = 120\n",
    "n_interior = 400\n",
    "\n",
    "# set up boundary points\n",
    "x_boundary = 2*torch.rand(n_bdry, 2).double() - 1\n",
    "I = torch.randint(2, size=(n_bdry,)).long()\n",
    "x_boundary[torch.arange(n_bdry), I] = 2*torch.randint(2, size=(n_bdry,)).double()-1\n",
    "\n",
    "# set up interior points\n",
    "x_interior = 2*torch.from_numpy(LatinHypercube(2).random(n_interior))-1\n",
    "x_interior.requires_grad = True\n",
    "\n",
    "# set up evaluation (grid) points\n",
    "n = 100\n",
    "xx = np.linspace(-1, 1, n)\n",
    "yy = np.linspace(-1, 1, n)\n",
    "gx, gy = np.meshgrid(xx, yy)\n",
    "x_eval = torch.from_numpy(np.stack([gx, gy]).reshape((2, n*n)).T)\n",
    "\n",
    "# plot the boundary points, interior points, and evaluation points\n",
    "plt.scatter(x_boundary.cpu().numpy()[:, 0], x_boundary.cpu().numpy()[:, 1], color='red', marker='x')\n",
    "plt.scatter(x_interior.detach().cpu().numpy()[:, 0], x_interior.detach().cpu().numpy()[:, 1], color='blue', marker='.')\n",
    "plt.scatter(x_eval.detach().cpu().numpy()[:, 0], x_eval.detach().cpu().numpy()[:, 1], color='green', marker='.', s=0.5)\n",
    "plt.xlim(-1.05, 1.05)\n",
    "plt.ylim(-1.05, 1.05)\n",
    "plt.gca().set_aspect('equal', 'box')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8ca9fc-922b-4a53-b7d9-ff6b14717efe",
   "metadata": {},
   "source": [
    "<h3>Question 3</h3>\n",
    "\n",
    "Implement the mean squared error boundary loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a90242-2939-4d48-825d-da5c295570f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_loss(network, bdry_pts, bdry_values):\n",
    "    \"\"\"\n",
    "    inputs: \n",
    "        network, a differentiable function (neural network) taking points (b, 2) to values (b, 1)\n",
    "        bdry_pts, a set of points (b, 2) on the boundary of the domain\n",
    "        bdry_values, a set of values of f (b, 1) at the bdry_pts\n",
    "\n",
    "    output:\n",
    "        a single number, the mean-squared-error of the network at the boundary\n",
    "    \"\"\"\n",
    "    \n",
    "    ##### evaluate network on the boundary points and do mean-squared-error loss against the ground truth values at those points ##### \n",
    "\n",
    "    \n",
    "\n",
    "    ##### ------------------------------------------------------------------------------------------------------- #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13e85f-b627-46b7-9480-3e037eb590d8",
   "metadata": {},
   "source": [
    "<h2>PDE Loss Definition</h2>\n",
    "\n",
    "In the interior of the domain, we evaluate a PDE loss which depends on the second derivative of the neural network.  Explicitly, if $u_\\theta$ is our neural network, our loss for the Laplace problem is defined as \n",
    "\n",
    "$$\\frac{1}{n_{\\text{interior}}}\\sum_{i=1}^{n_{\\text{interior}}}\\|\\frac{\\partial^2}{\\partial x^2}u_\\theta + \\frac{\\partial^2}{\\partial y^2}u_\\theta\\|^2$$\n",
    "\n",
    "This may seem weird: most standard data-scientific neural networks do not use the derivative of the network in the loss!  Fortunately, many automatic differentiation libraries are able to derive with respect to derivatives of a network.  Pytorch has a method called ```torch.autograd.grad``` for these purposes.\n",
    "\n",
    "<h3>Question 4</h3>\n",
    "\n",
    "Please research how to use ```torch.autograd.grad``` online.  Below, use ```torch.autograd.grad``` to implement the second partial derivatives in the x and y directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf2139-1285-4195-b3a0-89f279bf72ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_partial_x(u, pts):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        u, a differentiable function (/neural network) taking points (b, 2) to values (b, 1)\n",
    "        pts, a (b, 2) tensor representing the points to evaluate second derivatives at\n",
    "    outputs:\n",
    "        d2u, the second partial derivative of u in the x direction of shape (b, 1), evaluated at the points pts\n",
    "    \"\"\"\n",
    "\n",
    "    ##### YOUR CODE HERE #####\n",
    "\n",
    "    \n",
    "\n",
    "    ##### -------------- #####\n",
    "\n",
    "def double_partial_y(u, pts):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        u, a differentiable function (/neural network) taking points (b, 2) to values (b, 1)\n",
    "        pts, a (b, 2) tensor representing the points to evaluate second derivatives at\n",
    "    outputs:\n",
    "        d2u, the second partial derivative of u in the y direction of shape (b, 1), evaluated at the points pts\n",
    "    \"\"\"\n",
    "\n",
    "    ##### YOUR CODE HERE #####\n",
    "\n",
    "    \n",
    "\n",
    "    ##### -------------- #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19caeb49-ed7f-4fce-9f43-b85b92912f6d",
   "metadata": {},
   "source": [
    "You can test your implementation by running the following, which evaluates the second partial derivatives of the functions\n",
    "$$u_1(x, y) = 2.1x - 3.4y$$\n",
    "$$u_2(x, y) = x^2 + y^2$$\n",
    "$$u_3(x, y) = xy^3$$\n",
    "and finds the mean squared error.  The error should be very close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d9cf5-973a-436d-902f-0dd2bd27a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "u1 = lambda pts: torch.sum(torch.Tensor([[2.1, -3.4]])*pts**1, dim=1)\n",
    "u2 = lambda pts: pts[:, 0:1]**2 + pts[:, 1:2]**2\n",
    "u3 = lambda pts: pts[:, 0:1]*pts[:, 1:2]**3\n",
    "\n",
    "expected_ddx_u1 = lambda pts: torch.zeros((pts.shape[0], 1)).double()\n",
    "expected_ddy_u1 = lambda pts: torch.zeros((pts.shape[0], 1)).double()\n",
    "expected_ddx_u2 = lambda pts: 2*torch.ones((pts.shape[0], 1)).double()\n",
    "expected_ddy_u2 = lambda pts: 2*torch.ones((pts.shape[0], 1)).double()\n",
    "expected_ddx_u3 = lambda pts: torch.zeros((pts.shape[0], 1)).double()\n",
    "expected_ddy_u3 = lambda pts: 6*pts[:, 0:1]*pts[:, 1:2]\n",
    "\n",
    "print(\"u1 double partial x error\", torch.mean((double_partial_x(u1, x_interior) - expected_ddx_u1(x_interior))**2).item())\n",
    "print(\"u1 double partial y error\", torch.mean((double_partial_y(u1, x_interior) - expected_ddy_u1(x_interior))**2).item())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"u2 double partial x error\", torch.mean((double_partial_x(u2, x_interior) - expected_ddx_u2(x_interior))**2).item())\n",
    "print(\"u2 double partial y error\", torch.mean((double_partial_y(u2, x_interior) - expected_ddy_u2(x_interior))**2).item())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"u3 double partial x error\", torch.mean((double_partial_x(u3, x_interior) - expected_ddx_u3(x_interior))**2).item())\n",
    "print(\"u3 double partial y error\", torch.mean((double_partial_y(u3, x_interior) - expected_ddy_u3(x_interior))**2).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c612b9a5-0d74-4be6-b61e-847b093fc133",
   "metadata": {},
   "source": [
    "<h3>Question 5</h3>\n",
    "\n",
    "Write a function computing the PDE loss using the second derivative functions you just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc171ac-76af-4334-a5b5-1a2ec1371edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_pde_loss(u, pts):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        u, a differentiable function (/neural network) taking points (b, 2) to values (b, 1)\n",
    "        pts, points to evaluate the loss at\n",
    "    \"\"\"\n",
    "\n",
    "    ##### YOUR CODE HERE #####\n",
    "    \n",
    "    \n",
    "\n",
    "    ##### -------------- #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14173b69-65da-47ed-a87f-e780f446f7cf",
   "metadata": {},
   "source": [
    "Now train the PINN using the two losses.  Please keep the learning rate, etc the same and just run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab662f8-1aee-480b-b8e5-4568ce83a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60000\n",
    "lr = 2e-4\n",
    "lam_pde = 1.0\n",
    "test_step = 1000\n",
    "\n",
    "boundary_data = x_boundary[:, 0:1]**2 - x_boundary[:, 1:2]**2\n",
    "\n",
    "net = PINN().double()\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "losses = []\n",
    "eval_losses = []\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    b_loss = boundary_loss(net, x_boundary, boundary_data)\n",
    "    p_loss = laplace_pde_loss(net, x_interior)\n",
    "\n",
    "    loss = b_loss + lam_pde*p_loss\n",
    "\n",
    "    losses += [float(loss)]\n",
    "    \n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "plt.plot(np.arange(epochs), losses, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb31f13-98e7-47cb-bad3-761ee05c7376",
   "metadata": {},
   "source": [
    "Solutions to the Laplace equation are unique when boundary conditions are specified.  Therefore we expect the result to be the function $f(x,y)=x^2-y^2$, which we used to find the boundary conditions.  To check how well this is satisfied, we can evaluate on a different set of points and plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69c230-bdfa-49fb-a305-24e3729fe8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_u = net(x_eval)\n",
    "plt.scatter(x_eval.cpu().detach().numpy()[:, 0], x_eval.cpu().detach().numpy()[:, 1], c=eval_u.cpu().detach().numpy())\n",
    "plt.xlim(-1.05, 1.05)\n",
    "plt.ylim(-1.05, 1.05)\n",
    "plt.gca().set_aspect('equal', 'box')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca3477-fbd8-4991-9ab3-b9712983676e",
   "metadata": {},
   "source": [
    "Finally, we can plot the absolute error as well.  Your absolute error should be in the 1e-4 to 5e-3 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732e4084-5087-4dcf-a29c-186f52698789",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abs_error = np.abs(eval_u.cpu().detach().numpy() - (x_eval[:, 0:1]**2 - x_eval[:, 1:2]**2).cpu().detach().numpy())\n",
    "plt.scatter(x_eval.cpu().detach().numpy()[:, 0], x_eval.cpu().detach().numpy()[:, 1], c=abs_error)\n",
    "plt.xlim(-1.05, 1.05)\n",
    "plt.ylim(-1.05, 1.05)\n",
    "plt.gca().set_aspect('equal', 'box')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29dacfd-dcb1-4224-8718-4554cd88ea6b",
   "metadata": {},
   "source": [
    "<h3>Question 6</h3>\n",
    "What do you observe about the spatial distribution of the error?  Why do you think the spatial distribution of the error is like that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef27753-8077-4d16-b852-e96a21c25f8c",
   "metadata": {},
   "source": [
    "###\n",
    "\\#\\#\\#\\#\\# YOUR SOLUTION TYPED HERE\\#\\#\\#\\#\\#\n",
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
